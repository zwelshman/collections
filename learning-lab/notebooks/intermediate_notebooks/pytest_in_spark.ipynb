{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9yAjrEwkqqhrxeQDNhuKC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwelshman/collections/blob/main/learning-lab/notebooks/intermediate_notebooks/pytest_in_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "FFIDxsBIy8HT",
        "outputId": "780ccb22-7c48-4613-8f43-260ec3c70153"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7db2043ac340>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1fc8c8569b9a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pytest"
      ],
      "metadata": {
        "id": "PUUa-j6d0T6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a tests folder relative to the root directory"
      ],
      "metadata": {
        "id": "bOZx-AQbDEjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all tests in the repository root or directory.\n",
        "#Extract the directory from dirname\n",
        "root = os.path.dirname(os.path.abspath('.'))\n",
        "\n",
        "parent_dir = \"content\"\n",
        "\n",
        "# Tests Directory\n",
        "tests_dir = \"tests\"\n",
        "\n",
        "# Set path with root/tests_dir\n",
        "path = os.path.join(root, parent_dir, tests_dir)\n",
        "\n",
        "#Make the directory\n",
        "os.mkdir(path)\n",
        "\n",
        "print(f\"Directory {tests_dir} created\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vty6CzdKzjng",
        "outputId": "235dc8f6-bd76-47c6-c321-09e8dccff521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory tests created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing a test file with 2 functions to demonstrate a pytest assertion"
      ],
      "metadata": {
        "id": "HUQBEm9bBhH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_spark_functions.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def spark():\n",
        "  return SparkSession.builder.getOrCreate()\n",
        "\n",
        "def test_spark_pass(spark):\n",
        "  l = [('Alice', 1)]\n",
        "  assert spark.createDataFrame(l).collect() == [Row(_1='Alice', _2=1)]\n",
        "\n",
        "def test_spark_fail(spark):\n",
        "  l = [('Alice', 1)]\n",
        "  assert spark.createDataFrame(l).collect() == [Row(_1='Paul', _2=1)], 'Designed to fail'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYGqQDb1-sk9",
        "outputId": "860cc973-23f9-4eee-8e1c-b7d4f75a1c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_spark_functions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytest.main(['-x',\"-v\", 'tests'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF-ICYbB6QDn",
        "outputId": "9c4f0369-7840-4091-cad4-904ab49991f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================= test session starts ========================================\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.4.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "collecting ... collected 2 items\n",
            "\n",
            "tests/test_spark_functions.py::test_spark_pass PASSED                                        [ 50%]\n",
            "tests/test_spark_functions.py::test_spark_fail FAILED                                        [100%]\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "_________________________________________ test_spark_fail __________________________________________\n",
            "\n",
            "spark = <pyspark.sql.session.SparkSession object at 0x7db2043ac340>\n",
            "\n",
            "    def test_spark_fail(spark):\n",
            "      l = [('Alice', 1)]\n",
            ">     assert spark.createDataFrame(l).collect() == [Row(_1='Paul', _2=1)], 'Designed to fail'\n",
            "E     AssertionError: Designed to fail\n",
            "E     assert [Row(_1='Alice', _2=1)] == [Row(_1='Paul', _2=1)]\n",
            "E       At index 0 diff: Row(_1='Alice', _2=1) != Row(_1='Paul', _2=1)\n",
            "E       Full diff:\n",
            "E       - [Row(_1='Paul', _2=1)]\n",
            "E       ?          ^^^\n",
            "E       + [Row(_1='Alice', _2=1)]\n",
            "E       ?          ^ +++\n",
            "\n",
            "tests/test_spark_functions.py:15: AssertionError\n",
            "===================================== short test summary info ======================================\n",
            "FAILED tests/test_spark_functions.py::test_spark_fail - AssertionError: Designed to fail\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "=================================== 1 failed, 1 passed in 2.30s ====================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.TESTS_FAILED: 1>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDpctn_c-SzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}